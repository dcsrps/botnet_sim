{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ipaddress\n",
    "\n",
    "import sqlite3\n",
    "import sys\n",
    "import math\n",
    "from random import randint, choice, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_TO_ENCODE = \"/home/rhishi/Documents/tmp/fim_input_w_ts.csv\"\n",
    "#OUTPUT_PATTERN_DIR = \"/Users/admin/Documents/codes/FIM/data/\"\n",
    "#INPUT_DIR_TO_DECODE= \"/Users/admin/Downloads/labeled_output/Charm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Subnet and Time slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wo_subnet_timeshift(i_path,\\\n",
    "                       output_mapping_file ='/home/rhishi/Documents/tmp/mapping_wo_subnet_timeshift.txt',\\\n",
    "                       output_file_prefix = '/home/rhishi/Documents/tmp/wo_subnet'):\n",
    "    df_in=pd.read_csv(i_path, sep=',', dtype=str)\n",
    "\n",
    "    start_time = df_in['time'].astype(float).min() \n",
    "\n",
    "    df_in['time_diff'] = df_in['time'].astype(float) - start_time\n",
    "\n",
    "    cols_to_delete = ['dir', 'label', 'time', 'time_diff']\n",
    "\n",
    "    cols = df_in.columns.tolist()\n",
    "    data_col  = cols.copy()\n",
    "\n",
    "    for a_col in cols_to_delete:\n",
    "        data_col.remove(a_col)\n",
    "\n",
    "    for col in data_col:\n",
    "        df_in[col] = df_in[col].apply(lambda x: col+'_'+str(x))\n",
    "\n",
    "    uniq = []\n",
    "    for col in data_col:\n",
    "        uniq+=df_in[col].unique().tolist()\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(uniq)\n",
    "\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    with open(output_mapping_file, 'w') as file:\n",
    "        for k,v in le_name_mapping.items():\n",
    "            file.write(\"{} {}\\n\".format(k,v))\n",
    "\n",
    "    \"\"\"    \n",
    "    ################# with time step ######################\n",
    "    STEP = 300.0\n",
    "    ts = 0.0\n",
    "    count = 0    \n",
    "    while True:\n",
    "        df_entries_t = df_in[ (df_in['time_diff'] > ts) &  (df_in['time_diff'] <= ts+STEP)]\n",
    "        if df_entries_t.shape[0] > 0:\n",
    "            for flow_dir in ['0','1']:\n",
    "\n",
    "                df_entries = df_entries_t[df_entries_t['dir']==flow_dir][data_col]\n",
    "                bfr = []\n",
    "                for index,row in df_entries.iterrows():\n",
    "                    entry = row.tolist()\n",
    "                    bfr.append(le.transform(entry))\n",
    "\n",
    "                df_w = pd.DataFrame(bfr)\n",
    "                print('Writing {} {}'.format(flow_dir,ts))\n",
    "                df_w.to_csv(output_file_prefix+\"_dir_{}_step_{}.dat\".format(flow_dir,count),\\\n",
    "                            header=False, index=False, mode='w', sep=' ')\n",
    "            count+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        ts += STEP\n",
    "    \"\"\"\n",
    "        \n",
    "    # all entries without ts\n",
    "    for flow_dir in ['0','1']:\n",
    "        bfr = []\n",
    "        print('Direction: {}'.format(flow_dir))\n",
    "        df_entries = df_in[df_in['dir']==flow_dir][data_col]\n",
    "        for index,row in df_entries.iterrows():\n",
    "            entry = row.tolist()\n",
    "            bfr.append(le.transform(entry))\n",
    "\n",
    "        df_w = pd.DataFrame(bfr)\n",
    "        df_w.to_csv(output_file_prefix+\"_dir_{}_wo_subnet_all.dat\".format(flow_dir), index=False, mode='w', sep=' ')\n",
    "        \n",
    "    print('------------ Done ---------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Encoding with stepwise Timestamp (All entries till timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 300.0\n",
    "ts = STEP\n",
    "while True:\n",
    "    bfr = []\n",
    "    df_entries_t = df_in[ df_in['time_diff'] >= ts]\n",
    "        \n",
    "    if df_entries_t.shape[0] > 0:\n",
    "        for flow_dir in ['0','1']:\n",
    "            \n",
    "            df_entries = df_entries_t[df_entries_t['dir']==flow_dir][data_col]\n",
    "        \n",
    "            for index,row in df_entries.iterrows():\n",
    "                entry = row.tolist()\n",
    "                bfr.append(le.transform(entry))\n",
    "\n",
    "            df_w = pd.DataFrame(bfr)\n",
    "            print('Writing {} {}'.format(flow_dir,ts))\n",
    "            df_w.to_csv(\"/Users/admin/Downloads/tmp/formatted_dir_{}_upto_{}.dat\".format(flow_dir,ts),\\\n",
    "                        header=False, index=False, mode='w', sep=' ')\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "    ts += STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entries between timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wo_subnet_all(i_path,\\\n",
    "                       output_mapping_file ='/home/rhishi/Documents/tmp/mapping_wo_subnet_timeshift.txt',\\\n",
    "                       output_file_prefix = '/home/rhishi/Documents/tmp/wo_subnet'):\n",
    "for flow_dir in ['0','1']:\n",
    "    bfr = []\n",
    "    print('Direction: {}'.format(flow_dir))\n",
    "    df_entries = df_in[df_in['dir']==flow_dir][data_col]\n",
    "    for index,row in df_entries.iterrows():\n",
    "        entry = row.tolist()\n",
    "        bfr.append(le.transform(entry))\n",
    "    \n",
    "    df_w = pd.DataFrame(bfr)\n",
    "    df_w.to_csv(\"/tmp/formatted_dir_{}.dat\".format(flow_dir), index=False, mode='w', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_the_patterns(i_itemset, i_cols = ['iot', 'ext', 'sport', 'dport', 'sizeBin']):\n",
    "    ret_patterns= []\n",
    "    template_pattern = {}\n",
    "    for pattern in i_itemset: #self._final_itemset[i_flow_dir]:\n",
    "            \n",
    "        template_pattern = {}\n",
    "        for i in i_cols:\n",
    "            template_pattern[i]='*'\n",
    "\n",
    "        for value in pattern:\n",
    "            val = value.split('_')\n",
    "            if val[0] in i_cols:\n",
    "                template_pattern[val[0]] = val[1]\n",
    "                    \n",
    "        #print(pattern, template_pattern)\n",
    "        ret_patterns.append(template_pattern.values())\n",
    "\n",
    "    return template_pattern.keys(),ret_patterns\n",
    "\n",
    "def format_output(i_file):\n",
    "    all_patterns= []\n",
    "    with open(i_file, 'r') as rp:\n",
    "        for line in rp.readlines():\n",
    "            line = line.rstrip('\\n')\n",
    "            y = [int(i) for i in line.split(' ')[:-1]]\n",
    "            pattern=list(le.inverse_transform(y))\n",
    "            print(pattern)\n",
    "            all_patterns.append(pattern)\n",
    "\n",
    "        hdr, vals = fill_the_patterns(all_patterns)\n",
    "        df_w = pd.DataFrame(vals, columns=hdr)\n",
    "          \n",
    "        file_to_write = i_file+'_labeled'\n",
    "        df_w.to_csv(file_to_write, index=False, mode='w', sep=',') \n",
    "\n",
    "for filename in Path(INPUT_DIR_TO_DECODE).glob('*.txt'):\n",
    "    print('Doing file {}'.format(filename))\n",
    "    format_output(str(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Subnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_ip(i_ip, i_mask='16'):\n",
    "    try:\n",
    "        return str(ipaddress.IPv4Network(i_ip+\"/\"+i_mask, strict=False).network_address)\n",
    "    except:\n",
    "        return i_ip\n",
    "\n",
    "def w_subnet():\n",
    "    l_df=pd.read_csv(INPUT_FILE_TO_ENCODE, sep=',', dtype=str)\n",
    "\n",
    "    l_df['iot'] = list(map(get_masked_ip, l_df['iot']))\n",
    "    l_df['ext'] = list(map(get_masked_ip, l_df['ext']))\n",
    "\n",
    "    cols = l_df.columns.tolist()\n",
    "    data_col  = cols.copy()\n",
    "    data_col.remove('dir')\n",
    "    data_col.remove('label')          \n",
    "    for col in data_col:\n",
    "        l_df[col] = l_df[col].apply(lambda x: col+'_'+str(x))\n",
    "\n",
    "    uniq = []\n",
    "    for col in data_col:\n",
    "        uniq+=l_df[col].unique().tolist()\n",
    "\n",
    "    le_s = preprocessing.LabelEncoder()\n",
    "    le_s.fit(uniq)\n",
    "\n",
    "    le_name_mapping = dict(zip(le_s.classes_, le_s.transform(le_s.classes_)))\n",
    "\n",
    "    with open('/Users/admin/Downloads/mapping_w_subnet16s.txt', 'w') as file:\n",
    "        for k,v in le_name_mapping.items():\n",
    "            file.write(\"{} {}\\n\".format(k,v))\n",
    "\n",
    "    for flow_dir in ['0','1']:\n",
    "        bfr = []\n",
    "        print('Direction: {}'.format(flow_dir))\n",
    "        df_entries = l_df[l_df['dir']==flow_dir][data_col]\n",
    "        for index,row in df_entries.iterrows():\n",
    "            entry = row.tolist()\n",
    "            bfr.append(le_s.transform(entry))\n",
    "\n",
    "        df_w = pd.DataFrame(bfr)\n",
    "        df_w.to_csv(\"/Users/admin/Downloads/subnet16_dir_{}.dat\".format(flow_dir), index=False, mode='w', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  With Subnet and Time slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_ip(i_ip, i_mask='16'):\n",
    "    try:\n",
    "        return str(ipaddress.IPv4Network(i_ip+\"/\"+i_mask, strict=False).network_address)\n",
    "    except:\n",
    "        return i_ip\n",
    "\n",
    "def w_subnet_timeshift(i_path,\\\n",
    "                       output_mapping_file ='/home/rhishi/Documents/tmp/mapping_w_subnet_timeshift.txt',\\\n",
    "                       output_file_prefix = '/home/rhishi/Documents/tmp/w_subnet'):\n",
    "    l_df=pd.read_csv(i_path, sep=',', dtype=str)\n",
    "\n",
    "    l_df['iot'] = list(map(get_masked_ip, l_df['iot']))\n",
    "    l_df['ext'] = list(map(get_masked_ip, l_df['ext']))\n",
    "\n",
    "    start_time = l_df['time'].astype(float).min() \n",
    "\n",
    "    l_df['time_diff'] = l_df['time'].astype(float) - start_time\n",
    "\n",
    "    cols_to_delete = ['dir', 'label', 'time', 'time_diff']\n",
    "\n",
    "    cols = l_df.columns.tolist()\n",
    "    data_col  = cols.copy()\n",
    "    for a_col in cols_to_delete:\n",
    "        data_col.remove(a_col)\n",
    "\n",
    "    for col in data_col:\n",
    "        l_df[col] = l_df[col].apply(lambda x: col+'_'+str(x))\n",
    "\n",
    "    uniq = []\n",
    "    for col in data_col:\n",
    "        uniq+=l_df[col].unique().tolist()\n",
    "\n",
    "    le_s = preprocessing.LabelEncoder()\n",
    "    le_s.fit(uniq)\n",
    "\n",
    "    le_name_mapping = dict(zip(le_s.classes_, le_s.transform(le_s.classes_)))\n",
    "\n",
    "    with open(output_mapping_file, 'w') as file:\n",
    "        for k,v in le_name_mapping.items():\n",
    "            file.write(\"{} {}\\n\".format(k,v))\n",
    "\n",
    "    \"\"\"\n",
    "    # with time step\n",
    "    \n",
    "    STEP = 300.0\n",
    "    ts = 0.0\n",
    "    count = 0\n",
    "    while True:\n",
    "        df_entries_t = l_df[ (l_df['time_diff'] > ts) &  (l_df['time_diff'] <= ts+STEP)]\n",
    "\n",
    "        if df_entries_t.shape[0] > 0:\n",
    "            for flow_dir in ['0','1']:\n",
    "\n",
    "                df_entries = df_entries_t[df_entries_t['dir']==flow_dir][data_col]\n",
    "                bfr = []\n",
    "                for index,row in df_entries.iterrows():\n",
    "                    entry = row.tolist()\n",
    "                    bfr.append(le_s.transform(entry))\n",
    "\n",
    "                df_w = pd.DataFrame(bfr)\n",
    "                print('Writing {} {}'.format(flow_dir,ts))\n",
    "                df_w.to_csv(output_file_prefix+\"_dir_{}_step_{}.dat\".format(flow_dir,count),\\\n",
    "                            header=False, index=False, mode='w', sep=' ')\n",
    "            count+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        ts += STEP\n",
    "    \"\"\"   \n",
    "    # all entries without time slice\n",
    "    for flow_dir in ['0','1']:\n",
    "        bfr = []\n",
    "        print('Direction: {}'.format(flow_dir))\n",
    "        df_entries = l_df[l_df['dir']==flow_dir][data_col]\n",
    "        for index,row in df_entries.iterrows():\n",
    "            entry = row.tolist()\n",
    "            bfr.append(le_s.transform(entry))\n",
    "\n",
    "        df_w = pd.DataFrame(bfr)\n",
    "        df_w.to_csv(output_file_prefix+\"_dir_{}_w_subnet_all.dat\".format(flow_dir), index=False, mode='w', sep=' ')\n",
    "        \n",
    "    print('------------ Done ---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin(i_size):\n",
    "    if i_size <= 100:\n",
    "        return 'S'\n",
    "    elif i_size > 100 and i_size <=500:\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'L'\n",
    "    \n",
    "def get_all_entries(i_subsampling = True):\n",
    "    con = sqlite3.connect(\"/home/rhishi/workspace/nus/botnet_sim/manager/run_2/sim.db\")\n",
    "    crsr = con.cursor()\n",
    "    ret_list = []\n",
    "    event_count = {}\n",
    "\n",
    "    # subsample login and scan outs\n",
    "    \n",
    "    if i_subsampling:\n",
    "        login_attempts = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where pcount> 0 and dir = '1' ORDER BY RANDOM() LIMIT {}\".format(randint(2000,3000))\n",
    "        scan_outs = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where dport = '22' and dir = '0' ORDER BY RANDOM() LIMIT {}\".format(randint(2000,3000))\n",
    "    else:    \n",
    "        login_attempts = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where pcount> 0 and dir = '1'\"\n",
    "        scan_outs = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where dport = '22' and dir = '0'\"\n",
    "\n",
    "    scan_in = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where pcount = 0 and dir = '1' and extIP not like '10.2.%'\" \n",
    "    loader = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where dport = '8000' and dir = '0'\"\n",
    "    cnc = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where dport = '4567' and dir = '0'\"\n",
    "    http_attack = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where dport = '80' and dir = '0'\"\n",
    "    dns_attack = \"select intIP, extIP, proto, dport, dir, sport, insize/incount, outsize/outcount, time from sim where dport = '53' and dir = '0'\"\n",
    "\n",
    "    for q,l in zip([login_attempts,scan_outs,scan_in,loader,cnc,http_attack,dns_attack], ['login', 'scan-out', 'scan-in', 'loader', 'cnc', 'http_ddos', 'dns_rddos']): \n",
    "\n",
    "      crsr.execute(q)\n",
    "      l_op = crsr.fetchall()\n",
    "      if not l_op is None:\n",
    "            event_count[l] = len(l_op)\n",
    "            for entry in l_op:\n",
    "                try:\n",
    "                    insize = entry[6]\n",
    "                    outsize = entry[7]\n",
    "                    if entry[6] is None:\n",
    "                        insize = 0\n",
    "                    if entry[7] is None:\n",
    "                        outsize = 0\n",
    "                    size = int(insize+outsize)\n",
    "                    size_bin = get_bin(size)\n",
    "                    ret_list.append([entry[4], entry[0], entry[1], entry[2]+'.'+ entry[5],\\\n",
    "                        entry[2]+'.'+ entry[3], size_bin, entry[8], l])\n",
    "                except KeyError:\n",
    "                    print('Unknown direction entries in db.')\n",
    "    print(len(ret_list))\n",
    "    df_w = pd.DataFrame(ret_list)\n",
    "    df_w.to_csv('/home/rhishi/Documents/tmp/entries_reduced.csv', header=['dir', 'iot', 'ext', 'sport', 'dport', 'sizeBin', 'time', 'label'],index=False)\n",
    "    print(event_count)\n",
    "    con.close()\n",
    "\n",
    "\n",
    "get_all_entries(i_subsampling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ip_address(i_network):\n",
    "    net = ipaddress.IPv4Network(i_network)\n",
    "    return net[randint(2, net.num_addresses-2)].exploded\n",
    "\n",
    "\n",
    "def gen_noises(i_limit, output_file_name):\n",
    "    con = sqlite3.connect(\"/home/rhishi/workspace/nus/botnet_sim/manager/run_2/sim.db\")\n",
    "    crsr = con.cursor()\n",
    "    final_list = []\n",
    "    \n",
    "    LIMIT = randint(i_limit, i_limit+1000)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Add logic to add noise afterwards. \n",
    "        # get all the compromised iots.\n",
    "        iots = []\n",
    "        ports = [22,23,2323,443,80,8080]\n",
    "        crsr.execute('select distinct intIP from sim where dir =\"1\"')\n",
    "        l_op = crsr.fetchall()\n",
    "        for i in l_op :\n",
    "            iots.append(i[0])\n",
    "\n",
    "        crsr.execute(\"select min(time), max(time) from sim\")\n",
    "        min_time, max_time = crsr.fetchone()\n",
    "        print('Adding extra noise ......................')\n",
    "\n",
    "        while True:\n",
    "          iot = choice(iots)\n",
    "          ext = get_ip_address(\"0.0.0.0/0\")\n",
    "          proto = choice([6,17])\n",
    "          #dport = choice(ports)\n",
    "          dport = randint(20,1000)\n",
    "          sport = randint(11000,65000)\n",
    "          ts = uniform(float(min_time), float(max_time))\n",
    "          item_list = [iot, ext, proto, dport, 1, sport, 1, 1, 40, 40, 1, 0, ts]\n",
    "          where_clause_prefix = \"select incount from sim \"\n",
    "          where_clause = \"where \"\n",
    "          for i,j in enumerate(['intIP', 'extIP', 'proto', 'dport', 'dir']):\n",
    "            where_clause+= '{} = \"{}\" AND '.format(j, item_list[i])\n",
    "\n",
    "          #print(where_clause_prefix + where_clause[:-5])\n",
    "          crsr.execute(where_clause_prefix + where_clause[:-5])\n",
    "          l_op = crsr.fetchone()\n",
    "          \n",
    "          if l_op is None:\n",
    "            l_pattern = \"1,{},{},6.{},6.{},S,{},scan-in-noise\\n\".format(iot, ext, sport, dport, ts)\n",
    "            if not l_pattern in final_list: \n",
    "              #final_list.append(\",\".join(str(e) for e in item_list)+\"\\n\")\n",
    "              final_list.append(l_pattern)\n",
    "\n",
    "          if len(final_list) >= LIMIT:\n",
    "            break\n",
    "\n",
    "\n",
    "        print('Adding legit traffic ......................')\n",
    "\n",
    "        LIMIT1 = randint(30,50)\n",
    "        iots = []\n",
    "        crsr.execute('select distinct intIP from sim where dir = \"0\" and dport = \"443\"')\n",
    "        l_op = crsr.fetchall()\n",
    "        for i in l_op :\n",
    "            iots.append(i[0])\n",
    "\n",
    "        while True:\n",
    "          iot = choice(iots)\n",
    "          sport = randint(11000,65000)\n",
    "          ext = \"www.random.org.\"\n",
    "          dport = \"443\"\n",
    "          ts = uniform(min_time, max_time)\n",
    "          item_list = [iot, ext, 6, dport, 0, sport, 1, 1, 40, 40, 1, 0, ts]\n",
    "          where_clause_prefix = \"select incount from sim \"\n",
    "          where_clause = \"where \"\n",
    "          for i,j in enumerate(['intIP', 'extIP', 'proto', 'dport', 'dir', 'sport']):\n",
    "            where_clause+= '{} = \"{}\" AND '.format(j, item_list[i])\n",
    "\n",
    "          #print(where_clause_prefix + where_clause[:-5])\n",
    "          crsr.execute(where_clause_prefix + where_clause[:-5])\n",
    "          l_op = crsr.fetchone()\n",
    "          if l_op is None:\n",
    "           #final_list.append(\",\".join(str(e) for e in item_list)+\"\\n\")\n",
    "           final_list.append(\"0,{},{},6.{},6.{},L,{},legit\\n\".format(iot, ext, sport, dport, ts))\n",
    "\n",
    "          if len(final_list) >= LIMIT+LIMIT1:\n",
    "            break\n",
    "\n",
    "    except:\n",
    "      print('Exception.....')\n",
    "      print(sys.exc_info())\n",
    "\n",
    "    con.close()\n",
    "\n",
    "    with open(output_file_name, 'w') as fp:\n",
    "        fp.writelines(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_entries = 7866\n",
    "noise_muliple = 1\n",
    "gen_noises(total_entries*noise_muliple, \"/home/rhishi/Documents/tmp/extra_noise_1_w_ts.txt\")\n",
    "noise_muliple = 2\n",
    "gen_noises(total_entries*noise_muliple, \"/home/rhishi/Documents/tmp/extra_noise_2_w_ts.txt\")\n",
    "noise_muliple = 4\n",
    "gen_noises(total_entries*noise_muliple, \"/home/rhishi/Documents/tmp/extra_noise_3_w_ts.txt\")\n",
    "noise_muliple = 8\n",
    "gen_noises(total_entries*noise_muliple, \"/home/rhishi/Documents/tmp/extra_noise_4_w_ts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_1_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_1/mapping_noise_1_w_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_1/input\"\n",
    "                )\n",
    "\n",
    "w_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_2_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_2/mapping_noise_2_w_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_2/input\"\n",
    "                )\n",
    "\n",
    "w_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_3_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_3/mapping_noise_3_w_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_3/input\",\n",
    "                )\n",
    "\n",
    "w_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_4_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_4/mapping_noise_4_w_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_4/input\",\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_1_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_1/mapping_noise_1_wo_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_1/input\"\n",
    "                )\n",
    "\n",
    "wo_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_2_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_2/mapping_noise_2_wo_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_2/input\"\n",
    "                )\n",
    "\n",
    "wo_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_3_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_3/mapping_noise_3_wo_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_3/input\",\n",
    "                )\n",
    "\n",
    "wo_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_4_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_4/mapping_noise_4_wo_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_4/input\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_0_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_0/mapping_noise_0_w_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_0/input\")\n",
    "\n",
    "wo_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_0_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_0/mapping_noise_0_wo_subnet_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_0/input\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_no_cnc_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_1/mapping_noise_1_w_subnet_no_cnc_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/with_subnet/noise_1/input\")\n",
    "\n",
    "wo_subnet_timeshift(\"/home/rhishi/Documents/tmp/fim_input_no_cnc_w_ts.csv\",\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_1/mapping_noise_1_wo_subnet_no_cnc_timeshift.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/no_subnet/noise_1/input\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n"
     ]
    }
   ],
   "source": [
    "w_subnet_timeshift('/home/rhishi/Documents/tmp/submission/output/kushans_way/UNSW_attack_transactions.csv',\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/output/kushans_way/mapping_UNSW_w_subnet.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/output/kushans_way/input\")\n",
    "\n",
    "wo_subnet_timeshift('/home/rhishi/Documents/tmp/submission/output/kushans_way/UNSW_attack_transactions.csv',\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/output/kushans_way/mapping_UNSW_wo_subnet.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/output/kushans_way/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n",
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n"
     ]
    }
   ],
   "source": [
    "for noise in range(0,5):\n",
    "    print(\"Noise level -> {}\".format(noise))\n",
    "    for gateway in range(1,8):\n",
    "        w_subnet_timeshift('/tmp/gateway_wise/gateway_{}/gateway_{}_noise_{}'.format(gateway, gateway, noise),\n",
    "                   output_mapping_file = \"/tmp/gateway_wise/gateway_{}/mapping_gw_{}_noise_{}_w_subnet.txt\".format(gateway, gateway, noise),\n",
    "                   output_file_prefix = \"/tmp/gateway_wise/gateway_{}/input_gw_{}_noise_{}\".format(gateway, gateway, noise))\n",
    "\n",
    "        wo_subnet_timeshift('/tmp/gateway_wise/gateway_{}/gateway_{}_noise_{}'.format(gateway, gateway, noise),\n",
    "                   output_mapping_file = \"/tmp/gateway_wise/gateway_{}/mapping_gw_{}_noise_{}_wo_subnet.txt\".format(gateway, gateway, noise),\n",
    "                   output_file_prefix = \"/tmp/gateway_wise/gateway_{}/input_gw_{}_noise_{}\".format(gateway, gateway, noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_ip(i_ip, i_mask='16'):\n",
    "    try:\n",
    "        return str(ipaddress.IPv4Network(i_ip+\"/\"+i_mask, strict=False).network_address)\n",
    "    except:\n",
    "        return i_ip\n",
    "\n",
    "\n",
    "def timeshift_dataset_no_encode(i_path, i_subnet, output_file_prefix):\n",
    "    df_in=pd.read_csv(i_path, sep=',', dtype=str)\n",
    "    \n",
    "    if i_subnet == True:\n",
    "        print('With subnetting.')\n",
    "        df_in['iot'] = list(map(get_masked_ip, df_in['iot']))\n",
    "        df_in['ext'] = list(map(get_masked_ip, df_in['ext']))\n",
    "    else:\n",
    "        print('Without subnetting.')\n",
    "    \n",
    "    start_time = df_in['time'].astype(float).min() \n",
    "\n",
    "    df_in['time_diff'] = df_in['time'].astype(float) - start_time\n",
    "\n",
    "    cols_to_delete = ['dir', 'time', 'time_diff']\n",
    "\n",
    "    cols = df_in.columns.tolist()\n",
    "    data_col  = cols.copy()\n",
    "\n",
    "    for a_col in cols_to_delete:\n",
    "        data_col.remove(a_col)\n",
    "    \"\"\"\n",
    "    for col in data_col:\n",
    "        df_in[col] = df_in[col].apply(lambda x: col+'_'+str(x))\n",
    "    \"\"\"\n",
    "\n",
    "    uniq = []\n",
    "    for col in data_col:\n",
    "        uniq+=df_in[col].unique().tolist()\n",
    "\n",
    "    ################# with time step ######################\n",
    "    STEP = 300.0\n",
    "    ts = 0.0\n",
    "    count = 0    \n",
    "    while True:\n",
    "        df_entries_t = df_in[ (df_in['time_diff'] > ts) &  (df_in['time_diff'] <= ts+STEP)]\n",
    "        if df_entries_t.shape[0] > 0:\n",
    "                df_entries = df_entries_t[data_col]\n",
    "                \"\"\"\n",
    "                bfr = []\n",
    "                for index,row in df_entries.iterrows():\n",
    "                    entry = row.tolist()\n",
    "                    bfr.append(entry)\n",
    "\n",
    "                df_w = pd.DataFrame(bfr)\n",
    "                print('Writing {} {}'.format(flow_dir,ts))\n",
    "                \"\"\"\n",
    "                df_entries.to_csv(output_file_prefix+\"_dir_step_{}.dat\".format(count),\\\n",
    "                            header=False, index=False, mode='w', sep=' ')\n",
    "                count+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        ts += STEP\n",
    "        \n",
    "    print('------------ Done ---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without subnetting.\n",
      "------------ Done ---------------------\n",
      "With subnetting.\n",
      "------------ Done ---------------------\n"
     ]
    }
   ],
   "source": [
    "timeshift_dataset_no_encode('/home/rhishi/Documents/tmp/fim_input_0_w_ts.csv', i_subnet=False,\\\n",
    "                            output_file_prefix='/tmp/timestampwise_wo_subnet')\n",
    "timeshift_dataset_no_encode('/home/rhishi/Documents/tmp/fim_input_0_w_ts.csv', i_subnet=True,\\\n",
    "                            output_file_prefix='/tmp/timestampwise_w_subnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction: 0\n",
      "Direction: 1\n",
      "------------ Done ---------------------\n"
     ]
    }
   ],
   "source": [
    "wo_subnet_timeshift('/home/rhishi/Documents/tmp/paper_example.csv',\n",
    "                   output_mapping_file = \"/home/rhishi/Documents/tmp/submission/output/paper_mapping_wo_subnet.txt\",\n",
    "                   output_file_prefix = \"/home/rhishi/Documents/tmp/submission/output/paper_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
